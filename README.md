# Nashville Transportation Incentive Simulation

**Simulation-Based Evaluation of Incentive Mechanisms for Congestion Mitigation in Nashville Transportation Networks**

An agent-based simulation framework for evaluating incentive mechanisms to reduce urban congestion. This research develops novel computational methods for simulating strategic agent behavior, optimizing reward allocation, and calibrating behavioral models from real-world rideshare data.

## Research Objectives

This project investigates algorithmic approaches to incentive-based congestion mitigation, treating traffic participants as strategic agents whose behavior can be influenced through carefully designed reward mechanisms.

### Core Computer Science Contributions

- **Simulation Algorithm Design**: Event-driven agent-based simulation with spatial indexing for large-scale corridor simulations (10,000+ agents)
- **Incentive Optimization**: Approximation algorithms for optimal reward allocation under budget constraints
- **Behavioral Model Learning**: ML techniques to extract response functions from 369,831 historical rideshare trips
- **Equilibrium Computation**: Algorithms for computing Nash/Stackelberg equilibria in incentive-mediated systems

### Incentive Use Cases

| Use Case | Objective | Key Mechanism |
|----------|-----------|---------------|
| **Pacer Driving** | Reduce stop-and-go waves | Rewards for smooth speed profiles |
| **I-24 Carpooling** | Increase vehicle occupancy | Time/corridor-specific shared ride incentives |
| **Event Egress** | Flatten post-Titans game peaks | Departure delay & mode-shift rewards |
| **Transit Promotion** | Encourage mode shift | Geofenced peak-period transit incentives |

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Simulation Controller                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Agents     â”‚  â”‚  Incentive   â”‚  â”‚    Road Network      â”‚   â”‚
â”‚  â”‚  (Strategic) â”‚â—„â”€â”¤   Engine     â”‚â—„â”€â”¤  (Spatial Index)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Event-Driven Simulation Engine              â”‚   â”‚
â”‚  â”‚  â€¢ Priority Queue Scheduling                             â”‚   â”‚
â”‚  â”‚  â€¢ Spatial Hashing for Proximity Queries                 â”‚   â”‚
â”‚  â”‚  â€¢ Incremental State Updates                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Optimization â”‚  â”‚     ML       â”‚  â”‚    Analytics &       â”‚   â”‚
â”‚  â”‚  Algorithms  â”‚  â”‚  Calibration â”‚  â”‚    Validation        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
nashville-incentive-sim/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/              # Agent models and behavior
â”‚   â”‚   â”œâ”€â”€ base.py          # Base agent class
â”‚   â”‚   â”œâ”€â”€ commuter.py      # Commuter agent with mode/route choice
â”‚   â”‚   â”œâ”€â”€ pacer.py         # Flow-stabilizing pacer driver
â”‚   â”‚   â””â”€â”€ behavioral.py    # Behavioral response functions
â”‚   â”œâ”€â”€ incentives/          # Incentive mechanism implementations
â”‚   â”‚   â”œâ”€â”€ base.py          # Base incentive class
â”‚   â”‚   â”œâ”€â”€ carpool.py       # Carpooling incentives
â”‚   â”‚   â”œâ”€â”€ pacer.py         # Pacer driving rewards
â”‚   â”‚   â”œâ”€â”€ temporal.py      # Departure time shift incentives
â”‚   â”‚   â””â”€â”€ transit.py       # Transit promotion incentives
â”‚   â”œâ”€â”€ simulation/          # Core simulation engine
â”‚   â”‚   â”œâ”€â”€ engine.py        # Event-driven simulation controller
â”‚   â”‚   â”œâ”€â”€ events.py        # Event types and scheduling
â”‚   â”‚   â”œâ”€â”€ network.py       # Road network with spatial indexing
â”‚   â”‚   â””â”€â”€ metrics.py       # Performance measurement
â”‚   â”œâ”€â”€ optimization/        # Incentive optimization algorithms
â”‚   â”‚   â”œâ”€â”€ greedy.py        # Greedy allocation with approximation bounds
â”‚   â”‚   â”œâ”€â”€ dynamic.py       # Dynamic programming approaches
â”‚   â”‚   â”œâ”€â”€ metaheuristic.py # GA/simulated annealing
â”‚   â”‚   â””â”€â”€ online.py        # Online allocation algorithms
â”‚   â”œâ”€â”€ ml/                  # Machine learning for calibration
â”‚   â”‚   â”œâ”€â”€ features.py      # Feature engineering from GPS data
â”‚   â”‚   â”œâ”€â”€ models.py        # Classification/regression models
â”‚   â”‚   â””â”€â”€ validation.py    # Cross-validation and testing
â”‚   â”œâ”€â”€ data/                # Data loading and processing
â”‚   â”‚   â”œâ”€â”€ hytch.py         # Hytch rideshare data loader
â”‚   â”‚   â”œâ”€â”€ network.py       # Road network data (OSM)
â”‚   â”‚   â””â”€â”€ events.py        # Event schedules (Titans games)
â”‚   â””â”€â”€ utils/               # Utilities
â”‚       â”œâ”€â”€ config.py        # Configuration management
â”‚       â”œâ”€â”€ logging.py       # Structured logging
â”‚       â””â”€â”€ visualization.py # Plotting and dashboards
â”œâ”€â”€ tests/                   # Unit and integration tests
â”œâ”€â”€ notebooks/               # Jupyter notebooks for analysis
â”œâ”€â”€ configs/                 # YAML configuration files
â”œâ”€â”€ data/                    # Data directory (gitignored)
â”‚   â”œâ”€â”€ raw/                 # Raw input data
â”‚   â”œâ”€â”€ processed/           # Processed datasets
â”‚   â””â”€â”€ models/              # Trained ML models
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ scripts/                 # CLI scripts
â”œâ”€â”€ pyproject.toml           # Project configuration
â””â”€â”€ README.md
```

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/nashville-incentive-sim.git
cd nashville-incentive-sim

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -e ".[dev]"
```

### Run a Basic Simulation

```python
from src.simulation import SimulationEngine
from src.agents import CommuterAgent
from src.incentives import CarpoolIncentive

# Initialize simulation
engine = SimulationEngine.from_config("configs/i24_corridor.yaml")

# Add agents
engine.populate_agents(n_agents=5000, agent_class=CommuterAgent)

# Configure incentive mechanism
incentive = CarpoolIncentive(
    reward_per_passenger=2.0,
    peak_multiplier=1.5,
    budget_limit=10000
)
engine.add_incentive(incentive)

# Run simulation
results = engine.run(duration_hours=3, warmup_hours=0.5)

# Analyze results
print(f"Peak demand reduction: {results.peak_reduction_pct:.1f}%")
print(f"Average vehicle occupancy: {results.avg_occupancy:.2f}")
print(f"Incentive efficiency: ${results.cost_per_vmt_reduced:.2f}/VMT")
```

### Command Line Interface

```bash
# Run pacer simulation on I-24
python -m scripts.run_simulation --config configs/pacer_i24.yaml --agents 10000

# Train behavioral model from Hytch data
python -m scripts.train_model --data data/raw/hytch_trips.parquet --output data/models/

# Optimize incentive allocation
python -m scripts.optimize --scenario titans_game --budget 50000 --algorithm greedy

# Run full experiment suite
python -m scripts.run_experiments --suite all --output results/
```

## Data Sources

### Hytch Rideshare Data (Primary)

| Metric | Value |
|--------|-------|
| Total trips | 369,831 |
| Date range | 2018-2024 |
| Avg participants/trip | 2.5 |
| Training set | 352,354 trips (2018-2019) |
| Test set | 17,477 trips (2022-2024) |

Features extracted:
- Trip distance, duration, time-of-day
- Origin/destination zones
- Carpool formation rates
- Incentive response elasticity

### Nashville Road Network

- OpenStreetMap extract for Davidson County
- Focus corridors: I-24, I-40, I-65
- Spatial indexing with R-tree for proximity queries

### Event Data

- Titans game schedules (Nissan Stadium)
- Concert and event calendar
- Historical traffic patterns from TDOT

## ðŸ”¬ Methodology

### Agent Behavioral Models

Agents make decisions using bounded-rational utility maximization:

```
U(action) = Î²â‚€ + Î²â‚Â·travel_time + Î²â‚‚Â·cost + Î²â‚ƒÂ·incentive + Î²â‚„Â·comfort + Îµ
```

Decision rules:
- **Softmax**: P(action) âˆ exp(U(action)/Ï„)
- **Epsilon-greedy**: Explore with probability Îµ
- **Best response**: Pure utility maximization

### Incentive Optimization

Budget-constrained allocation problem:

```
maximize    Î£áµ¢ congestion_reduction(incentiveáµ¢)
subject to  Î£áµ¢ costáµ¢ â‰¤ B (budget)
            incentiveáµ¢ â‰¥ 0
```

Algorithms implemented:
| Algorithm | Approximation Ratio | Time Complexity |
|-----------|---------------------|-----------------|
| Greedy | 1 - 1/e â‰ˆ 0.63 | O(n log n) |
| Dynamic Programming | Optimal (pseudo-poly) | O(nB) |
| Genetic Algorithm | Empirical | O(gÂ·pÂ·n) |
| Online (Secretary) | 1/e â‰ˆ 0.37 | O(n) |

### Equilibrium Computation

For multi-agent strategic interactions:
- **Best Response Dynamics**: Iterate until convergence
- **Fictitious Play**: Learn from historical play
- **Potential Games**: Exploit structure for faster convergence

## Evaluation Metrics

### Computational Metrics
- Runtime complexity (wall-clock, big-O)
- Memory footprint and scalability
- Convergence rate to equilibrium
- Approximation quality

### Transportation Metrics
- Peak demand reduction (%)
- Travel time reliability (95th percentile)
- Vehicle-miles traveled (VMT) reduction
- Average vehicle occupancy
- Incentive efficiency ($/VMT reduced)

### Validation Metrics
- Prediction accuracy (AUC, RMSE)
- Behavioral model fit (Ï‡Â², KS test)
- Simulation-to-reality gap

## Experiments

### Experiment 1: Pacer Participation Threshold

**Question**: What minimum pacer participation rate yields measurable congestion reduction?

```bash
python -m scripts.run_experiments --experiment pacer_threshold \
    --participation-rates 0.01 0.02 0.05 0.10 0.15 0.20 \
    --replications 30
```

### Experiment 2: Carpool Incentive Elasticity

**Question**: Is it more effective to increase reward magnitude or targeting precision?

```bash
python -m scripts.run_experiments --experiment carpool_elasticity \
    --reward-levels 1.0 2.0 5.0 10.0 \
    --targeting-precision low medium high
```

### Experiment 3: Event Egress Optimization

**Question**: Are small delays across many participants more effective than large delays for few?

```bash
python -m scripts.run_experiments --experiment event_egress \
    --delay-distribution uniform concentrated \
    --total-delay-budget 1000
```

## Configuration

Example configuration file (`configs/i24_corridor.yaml`):

```yaml
simulation:
  name: "I-24 Carpooling Study"
  duration_hours: 4
  warmup_hours: 0.5
  time_step_seconds: 1.0
  random_seed: 42

network:
  source: "osm"
  bounds:
    north: 36.20
    south: 36.05
    east: -86.60
    west: -86.90
  corridors:
    - name: "I-24"
      osm_relation_id: 123456

agents:
  total: 10000
  types:
    commuter:
      fraction: 0.85
      behavioral_model: "softmax"
      temperature: 0.5
    pacer_eligible:
      fraction: 0.15
      compliance_rate: 0.7

incentives:
  - type: "carpool"
    enabled: true
    reward_per_passenger: 2.50
    peak_hours: [7, 8, 9, 17, 18, 19]
    peak_multiplier: 1.5
    budget_daily: 10000

  - type: "pacer"
    enabled: true
    reward_per_mile: 0.10
    smoothness_threshold: 0.8
    corridor_segments: ["I-24-seg-1", "I-24-seg-2"]

optimization:
  algorithm: "greedy"
  budget_constraint: 50000
  objective: "vmt_reduction"

output:
  directory: "results/i24_carpool"
  save_trajectories: false
  save_metrics_interval: 300
  plots: ["demand_profile", "speed_heatmap", "incentive_uptake"]
```

## Development

### Running Tests

```bash
# All tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=src --cov-report=html

# Specific test module
pytest tests/test_simulation.py -v
```

### Code Quality

```bash
# Format code
black src/ tests/

# Lint
ruff check src/ tests/

# Type checking
mypy src/
```

### Building Documentation

```bash
cd docs/
make html
```

### Gradio Dashboard Deployment

**Local Development:**
```bash
# Run locally
cd app && python app.py

# Run with auto-reload for development
cd app && gradio app.py
```

**Deploy to Hugging Face Spaces:**
1. Create a new Space at [huggingface.co/new-space](https://huggingface.co/new-space)
2. Select "Gradio" as the SDK
3. Clone your Space repository:
   ```bash
   git clone https://huggingface.co/spaces/YOUR_USERNAME/YOUR_SPACE_NAME
   ```
4. Copy the app files:
   ```bash
   cp -r app/* YOUR_SPACE_NAME/
   cp warehouse.duckdb YOUR_SPACE_NAME/  # Include database
   ```
5. Create `requirements.txt` in the Space:
   ```
   gradio>=4.0.0
   plotly>=5.0.0
   duckdb>=0.9.0
   pandas>=2.0.0
   ```
6. Push to deploy:
   ```bash
   cd YOUR_SPACE_NAME
   git add . && git commit -m "Deploy dashboard" && git push
   ```

**Live Demo:** https://huggingface.co/spaces/LeonceNsh/nashville-incentive-simulation

## References

### Algorithmic Game Theory
- Nisan et al. (2007). *Algorithmic Game Theory*. Cambridge University Press.
- Roughgarden (2016). *Twenty Lectures on Algorithmic Game Theory*. Cambridge.

### Traffic Flow Theory
- Treiber & Kesting (2013). *Traffic Flow Dynamics*. Springer.
- Stern et al. (2018). Dissipation of stop-and-go waves via control of autonomous vehicles. *Transportation Research Part C*.

### Incentive Mechanism Design
- Mirrokni et al. (2012). Optimal marketing strategies over social networks. *WWW*.
- Chen et al. (2015). Peeking beneath the hood of Uber. *IMC*.

## Contributors

- **Leonce Nshuti** - Primary Researcher - Vanderbilt University
- **MobileFlow/Hytch** - External Collaborator - Domain expertise and data access
